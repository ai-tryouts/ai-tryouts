# Wikipedia Semantic Search Engine with Streamlit

This project demonstrates a simple semantic search engine built using Streamlit, Sentence Transformers, and Hugging Face Datasets. It allows users to load Wikipedia articles, generate text embeddings, save/load these embeddings, and perform semantic search to find articles most similar to a given query. The application also showcases the internal calculation steps involved in the search process.

## Features

-   Load a subset of Wikipedia articles.
-   Generate sentence embeddings using the `all-MiniLM-L6-v2` model.
-   Save generated embeddings and corresponding texts locally for faster subsequent runs.
-   Load pre-existing embeddings and texts if available.
-   User interface to input search queries.
-   Calculates cosine similarity between the query embedding and all article embeddings.
-   Displays the top 5 most similar articles.
-   Measures and displays query time.
-   Provides a detailed breakdown of the search calculation steps on the frontend:
    -   Query embedding.
    -   Cosine similarity formula.
    -   All similarity scores (expandable).
    -   Ranking information.
    -   Detailed cosine similarity calculation components for each top result (expandable).

## Setup and Installation

### 1. Create a Virtual Environment

It's recommended to use a virtual environment to manage project dependencies. If you have `bert-env` already set up from previous steps, you can activate it. Otherwise, create a new one:

```bash
python3 -m venv bert-env 
# or python -m venv bert-env
```

Activate the virtual environment:

**On macOS and Linux:**
```bash
source bert-env/bin/activate
```

**On Windows:**
```bash
bert-env\Scripts\activate
```

### 2. Install Dependencies

Once the virtual environment is activated, install the required Python packages:

```bash
pip install sentence-transformers streamlit datasets scikit-learn numpy
```

## Running the Application

1.  Ensure your virtual environment is activated.
2.  Navigate to the project directory (where `app.py` is located) in your terminal.
3.  Run the Streamlit application:

    ```bash
    streamlit run app.py
    ```

4.  The application should automatically open in your web browser. If not, navigate to `http://localhost:8501`.

## How It Works

1.  **Data Loading:**
    -   On the first run (or if local files `wikipedia_texts.npy` and `wikipedia_embeddings.npy` are not found), the app loads a small subset of Wikipedia articles (first 100 from `train[:1%]` of `wikipedia:20220301.en`).
    -   If these `.npy` files exist in the project directory, the app loads texts and embeddings from them for faster startup.

2.  **Embedding Generation:**
    -   The `all-MiniLM-L6-v2` model from Sentence Transformers is used to convert article texts into numerical embeddings (vectors).
    -   Users can click "Generate and Save Embeddings Now" to process the loaded texts, generate embeddings, and save both `wikipedia_texts.npy` and `wikipedia_embeddings.npy`.

3.  **Semantic Search:**
    -   The user enters a search query.
    -   The query is converted into an embedding using the same model.
    -   Cosine similarity is calculated between the query embedding and all loaded document embeddings.
    -   The documents are ranked by their similarity scores, and the top 5 are displayed.
    -   Detailed calculation steps, including the query embedding, similarity scores, and components of cosine similarity for top results, are shown to provide insight into the process.

## Files

-   `app.py`: The main Streamlit application script.
-   `wikipedia_embeddings.npy`: (Generated by the app) Stores the NumPy array of document embeddings.
-   `wikipedia_texts.npy`: (Generated by the app) Stores the NumPy array of document texts.
-   `README.md`: This file.
